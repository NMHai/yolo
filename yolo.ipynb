{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giới thiệu\n",
    "Trong notebook này, mình sẽ hướng dẫn các bạn cài đặt mô hình YOLO v1 cho tập dữ liệu mẫu được phát sinh. Sau khi làm xong, các bạn sẽ nắm được mô hình CNN, và ý nghĩa hàm loss, cách cài đặt, cũng như quá trình huấn luyện bằng tensorflow\n",
    "\n",
    "You only look once (YOLO) là một mô hình CNN để detect object mà ưu điểm nổi trội là nhanh hơn nhiều so với những mô hình cũ. Thậm chí có thể chạy tốt trên những thiết bị IOT có cấu hình yếu như raspberry pi.\n",
    "\n",
    "Đầu vào mô hình là một bức ảnh, đối với bài toán object detection, chúng ta không chỉ phải phân loại được object trên bức ảnh mà còn phải định vị được vị trí của đối tượng đó. Object Detection có khá nhiều ứng dụng, ví dụ như hệ thống theo dõi người dẫn của Trung Quốc, từ đó có thể giúp chính quyền xác định được tội phạm lẫn trốn ở đó hay không, hoặc hệ thống xe tự lái, cũng phải xác định được người đi đường ở đâu từ đó đưa ra quyết định di chuyển tiếp theo\n",
    "\n",
    "![yolo](https://pbcquoc.github.io/images/yolo_example.png)\n",
    "\n",
    "Có một số hướng tiếp cận để giải quyết vấn đề, đồng thời mỗi lần chạy tốn rất nhiều thời gian. Mình sẽ liệt kê ra để các bạn có thể nắm được ý tưởng để giải quyết bài toàn object detection.\n",
    "\n",
    "* Chia ảnh thành nhiều box, mỗi box các bạn sẽ detect object trong box đó. Vị trí của object chính là tạo độ của box đó.\n",
    "* Thay vì chia thành từng box, chúng ta sẽ sử dụng một thuật toán để lựa chọn những region ứng viên (ví dụ như là thuật toán Selective Search), các vùng ứng viên này các bạn có thể tưởng như là những vùng liên thông với nhau trên kênh màu RGB, sau đó với mỗi vùng ứng viên này, chúng ta dùng model để phân loại object. Chúng ta có một số mô hình xây dựng theo kiểu này như RCNN, Fast RCNN và Faster RCNN.\n",
    "\n",
    "Rất rõ ràng, nhược điểm của các phương pháp trên là tốn rất nhiều tài nguyên để tính toán cho mội vùng trên một bức ảnh,và do đó không thể chạy realtime trên các thiết bị yếu.\n",
    "\n",
    "### import các thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## slim là package đi kèm với tensorflow, giúp định nghĩa nhanh các loại mô hình deep learning\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.slim.nets import vgg \n",
    "## sklearn là một thư viện rất phổ biến trong ML, chúng ta chỉ sử dụng tran_test_split để chia data thành 2 tập\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "## thư viện tính toán trên matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "# thư viện hiển thị biểu đồ\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid System\n",
    "\n",
    "Ảnh được chia thành ma trận ô vuông 7x7, mỗi ô vuông bao gồm một tập các thông tin mà mô hình phải dữ đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kích thước grid system \n",
    "cell_size = 7 \n",
    "# số boundary box cần dự đoán mỗi ô vuông\n",
    "box_per_cell = 2\n",
    "# kích thước ảnh đầu vào\n",
    "img_size = 224\n",
    "# số loại nhãn\n",
    "classes = {'circle':0, 'triangle':1,  'rectangle':2}\n",
    "nclass = len(classes)\n",
    "\n",
    "box_scale = 5.0\n",
    "noobject_scale = 0.5\n",
    "batch_size = 128\n",
    "# số lần huấn luyện\n",
    "epochs = 100\n",
    "# learning của chúng ta\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    labels = json.load(open('../data/yolo/train/labels.json'))\n",
    "    # số lương ảnh\n",
    "    N = len(labels)\n",
    "    # matrix chứa ảnh\n",
    "    X = np.zeros((N, img_size, img_size, 3), dtype='uint8')\n",
    "    # matrix chứa nhãn của ảnh tương ứng\n",
    "    y = np.zeros((N,cell_size, cell_size, 5+nclass))\n",
    "    for idx, label in enumerate(labels):\n",
    "        img = cv2.imread(\"../data/yolo/train/{}.png\".format(idx))\n",
    "        # normalize về khoảng [0-1]\n",
    "        X[idx] = img/255.0\n",
    "        for box in label['boxes']:\n",
    "            x1, y1 = box['x1'], box['y1']\n",
    "            x2, y2 = box['x2'], box['y2']\n",
    "            # one-hot vector của nhãn object\n",
    "            cl = [0]*len(classes)\n",
    "            cl[classes[box['class']]] = 1\n",
    "            # tâm của boundary box\n",
    "            x_center, y_center, w, h = (x1+x2)/2.0, (y1+y2)/2.0, x2-x1, y2-y1\n",
    "            # index của object trên ma trận ô vuông 7x7\n",
    "            x_idx, y_idx = int(x_center/img_size*cell_size), int(y_center/img_size*cell_size)\n",
    "            # gán nhãn vào matrix \n",
    "            y[idx, y_idx, x_idx] = 1, x_center, y_center, w, h, *cl\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chia tập dữ liệu thành train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2, scope='iou'):\n",
    "    \"\"\"calculate ious\n",
    "    Args:\n",
    "      boxes1: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "      boxes2: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===> (x_center, y_center, w, h)\n",
    "    Return:\n",
    "      iou: 4-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # transform (x_center, y_center, w, h) to (x1, y1, x2, y2)\n",
    "        boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
    "                             boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
    "                             boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
    "                             boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
    "                            axis=-1)\n",
    "\n",
    "        boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
    "                             boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
    "                             boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
    "                             boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
    "                            axis=-1)\n",
    "\n",
    "        # calculate the left up point & right down point\n",
    "        lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
    "        rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
    "\n",
    "        # intersection\n",
    "        intersection = tf.maximum(0.0, rd - lu)\n",
    "        inter_square = intersection[..., 0] * intersection[..., 1]\n",
    "\n",
    "        # calculate the boxs1 square and boxs2 square\n",
    "        square1 = boxes1[..., 2] * boxes1[..., 3]\n",
    "        square2 = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "        union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n",
    "\n",
    "    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(inputs, is_training):\n",
    "    with tf.variable_scope(\"vgg_16\"):\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 16, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 32, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 64, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "            net = slim.conv2d(net, 512, [1, 1], scope='fc6')   \n",
    "\n",
    "            net = slim.conv2d(net, 13, [1, 1], activation_fn=None, scope='fc7')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_layer(predicts, labels, scope='loss_layer'):\n",
    "    with tf.variable_scope(scope):\n",
    "        offset = np.transpose(np.reshape(np.array(\n",
    "            [np.arange(cell_size)] * cell_size * box_per_cell),\n",
    "            (box_per_cell, cell_size, cell_size)), (1, 2, 0))\n",
    "        offset = offset[None, :]\n",
    "        offset = tf.constant(offset, dtype=tf.float32)\n",
    "        offset_tran = tf.transpose(offset, (0, 2, 1, 3))\n",
    "        predict_object = predicts[..., :box_per_cell]\n",
    "        predict_box_offset = tf.reshape(predicts[...,box_per_cell:5*box_per_cell], (-1, cell_size, cell_size, box_per_cell, 4))\n",
    "        predict_class = predicts[...,5*box_per_cell:]\n",
    "        predict_normalized_box = tf.stack(\n",
    "                                    [(predict_box_offset[..., 0] + offset) / cell_size,\n",
    "                                     (predict_box_offset[..., 1] + offset_tran) / cell_size,\n",
    "                                     tf.square(predict_box_offset[..., 2]),\n",
    "                                    tf.square(predict_box_offset[..., 3])], axis=-1)\n",
    "        \n",
    "        true_object = labels[..., :1]\n",
    "        true_box = tf.reshape(labels[..., 1:5], (-1, cell_size, cell_size, 1, 4))\n",
    "        true_normalized_box = tf.tile(true_box, (1, 1, 1, box_per_cell, 1))/img_size\n",
    "        true_class = labels[..., 5:]\n",
    "        true_box_offset =  tf.stack(\n",
    "                                    [true_normalized_box[..., 0] * cell_size - offset,\n",
    "                                     true_normalized_box[..., 1] * cell_size - offset_tran,\n",
    "                                     tf.sqrt(true_normalized_box[..., 2]),\n",
    "                                     tf.sqrt(true_normalized_box[..., 3])], axis=-1)\n",
    "        \n",
    "        predict_iou = compute_iou(true_normalized_box, predict_normalized_box)\n",
    "        \n",
    "        object_mask = tf.reduce_max(predict_iou, 3, keepdims=True)  \n",
    "        \n",
    "        iou_metric = tf.reduce_mean(tf.reduce_sum(object_mask, axis=[1,2,3])/tf.reduce_sum(true_object, axis=[1,2,3]))\n",
    "        \n",
    "        object_mask = tf.cast((predict_iou>=object_mask), tf.float32)*true_object\n",
    "\n",
    "        noobject_mask = tf.ones_like(object_mask) - object_mask\n",
    "        \n",
    "        ## class loss\n",
    "        class_delta = true_object*(predict_class - true_class)\n",
    "        class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1,2,3]), name='class_loss')\n",
    "        ## object loss\n",
    "        object_delta = object_mask*(predict_object - predict_iou)\n",
    "        object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1,2,3]), name='object_loss')\n",
    "        ## noobject loss\n",
    "        noobject_delta = noobject_mask*predict_object\n",
    "        noobject_loss = tf.reduce_mean(tf.reduce_sum(tf.square(noobject_delta), axis=[1,2,3]), name='noobject_loss')\n",
    "        ## coord loss\n",
    "        box_mask = tf.expand_dims(object_mask, 4)\n",
    "        box_delta = box_mask*(predict_box_offset - true_box_offset)\n",
    "        box_loss = tf.reduce_mean(tf.reduce_sum(tf.square(box_delta), axis=[1,2,3]), name='box_loss')\n",
    "        \n",
    "        loss = 0.5*class_loss + object_loss + 0.1*noobject_loss + 10*box_loss\n",
    "        \n",
    "        return loss, iou_metric, predict_object, predict_class, predict_normalized_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():    \n",
    "    images = tf.placeholder(\"float\", [None, img_size, img_size, 3], name=\"input\")\n",
    "    labels = tf.placeholder('float', [None, cell_size, cell_size, 8], name='label')\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    logits = vgg16(images, is_training)\n",
    "    loss, iou_metric, predict_object, predict_class, predict_normalized_box = loss_layer(logits, labels)\n",
    "            \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - running_time: 94s - train_loss: 1.347 - train_iou: 0.392 - val_loss: 1.289 - val_iou: 0.509\n",
      "epoch: 1 - running_time: 88s - train_loss: 0.323 - train_iou: 0.610 - val_loss: 0.319 - val_iou: 0.595\n",
      "epoch: 2 - running_time: 87s - train_loss: 0.188 - train_iou: 0.653 - val_loss: 0.210 - val_iou: 0.669\n",
      "epoch: 3 - running_time: 89s - train_loss: 0.160 - train_iou: 0.653 - val_loss: 0.186 - val_iou: 0.642\n",
      "epoch: 4 - running_time: 89s - train_loss: 0.130 - train_iou: 0.723 - val_loss: 0.164 - val_iou: 0.663\n",
      "epoch: 5 - running_time: 89s - train_loss: 0.127 - train_iou: 0.796 - val_loss: 0.143 - val_iou: 0.701\n",
      "epoch: 6 - running_time: 89s - train_loss: 0.124 - train_iou: 0.785 - val_loss: 0.128 - val_iou: 0.754\n",
      "epoch: 7 - running_time: 89s - train_loss: 0.084 - train_iou: 0.804 - val_loss: 0.107 - val_iou: 0.790\n",
      "epoch: 8 - running_time: 89s - train_loss: 0.110 - train_iou: 0.773 - val_loss: 0.127 - val_iou: 0.769\n",
      "epoch: 9 - running_time: 89s - train_loss: 0.076 - train_iou: 0.825 - val_loss: 0.097 - val_iou: 0.812\n",
      "epoch: 10 - running_time: 88s - train_loss: 0.065 - train_iou: 0.819 - val_loss: 0.091 - val_iou: 0.815\n",
      "epoch: 11 - running_time: 89s - train_loss: 0.061 - train_iou: 0.831 - val_loss: 0.090 - val_iou: 0.833\n",
      "epoch: 12 - running_time: 89s - train_loss: 0.054 - train_iou: 0.854 - val_loss: 0.086 - val_iou: 0.839\n",
      "epoch: 13 - running_time: 90s - train_loss: 0.052 - train_iou: 0.847 - val_loss: 0.089 - val_iou: 0.844\n",
      "epoch: 14 - running_time: 90s - train_loss: 0.080 - train_iou: 0.781 - val_loss: 0.105 - val_iou: 0.804\n",
      "epoch: 15 - running_time: 89s - train_loss: 0.048 - train_iou: 0.846 - val_loss: 0.084 - val_iou: 0.845\n",
      "epoch: 16 - running_time: 88s - train_loss: 0.046 - train_iou: 0.850 - val_loss: 0.082 - val_iou: 0.830\n",
      "epoch: 17 - running_time: 88s - train_loss: 0.055 - train_iou: 0.843 - val_loss: 0.093 - val_iou: 0.800\n",
      "epoch: 18 - running_time: 89s - train_loss: 0.040 - train_iou: 0.862 - val_loss: 0.081 - val_iou: 0.845\n",
      "epoch: 19 - running_time: 89s - train_loss: 0.042 - train_iou: 0.853 - val_loss: 0.098 - val_iou: 0.811\n",
      "epoch: 20 - running_time: 89s - train_loss: 0.039 - train_iou: 0.860 - val_loss: 0.081 - val_iou: 0.848\n",
      "epoch: 21 - running_time: 89s - train_loss: 0.035 - train_iou: 0.875 - val_loss: 0.079 - val_iou: 0.859\n",
      "epoch: 22 - running_time: 89s - train_loss: 0.033 - train_iou: 0.879 - val_loss: 0.079 - val_iou: 0.853\n",
      "epoch: 23 - running_time: 89s - train_loss: 0.035 - train_iou: 0.873 - val_loss: 0.081 - val_iou: 0.863\n",
      "epoch: 24 - running_time: 89s - train_loss: 0.035 - train_iou: 0.870 - val_loss: 0.080 - val_iou: 0.849\n",
      "epoch: 25 - running_time: 87s - train_loss: 0.033 - train_iou: 0.882 - val_loss: 0.074 - val_iou: 0.869\n",
      "epoch: 26 - running_time: 89s - train_loss: 0.040 - train_iou: 0.859 - val_loss: 0.079 - val_iou: 0.855\n",
      "epoch: 27 - running_time: 89s - train_loss: 0.034 - train_iou: 0.883 - val_loss: 0.081 - val_iou: 0.860\n",
      "epoch: 28 - running_time: 89s - train_loss: 0.031 - train_iou: 0.879 - val_loss: 0.081 - val_iou: 0.852\n",
      "epoch: 29 - running_time: 89s - train_loss: 0.032 - train_iou: 0.876 - val_loss: 0.081 - val_iou: 0.864\n",
      "epoch: 30 - running_time: 89s - train_loss: 0.030 - train_iou: 0.886 - val_loss: 0.077 - val_iou: 0.863\n",
      "epoch: 31 - running_time: 89s - train_loss: 0.031 - train_iou: 0.886 - val_loss: 0.076 - val_iou: 0.870\n",
      "epoch: 32 - running_time: 89s - train_loss: 0.033 - train_iou: 0.876 - val_loss: 0.080 - val_iou: 0.854\n",
      "epoch: 33 - running_time: 89s - train_loss: 0.041 - train_iou: 0.860 - val_loss: 0.087 - val_iou: 0.836\n",
      "epoch: 34 - running_time: 89s - train_loss: 0.030 - train_iou: 0.887 - val_loss: 0.078 - val_iou: 0.870\n",
      "epoch: 35 - running_time: 89s - train_loss: 0.030 - train_iou: 0.884 - val_loss: 0.080 - val_iou: 0.869\n",
      "epoch: 36 - running_time: 89s - train_loss: 0.031 - train_iou: 0.876 - val_loss: 0.079 - val_iou: 0.879\n",
      "epoch: 37 - running_time: 89s - train_loss: 0.028 - train_iou: 0.890 - val_loss: 0.078 - val_iou: 0.872\n",
      "epoch: 38 - running_time: 89s - train_loss: 0.127 - train_iou: 0.778 - val_loss: 0.114 - val_iou: 0.759\n",
      "epoch: 39 - running_time: 89s - train_loss: 0.068 - train_iou: 0.838 - val_loss: 0.091 - val_iou: 0.820\n",
      "epoch: 40 - running_time: 89s - train_loss: 0.047 - train_iou: 0.849 - val_loss: 0.090 - val_iou: 0.831\n",
      "epoch: 41 - running_time: 89s - train_loss: 0.036 - train_iou: 0.863 - val_loss: 0.080 - val_iou: 0.851\n",
      "epoch: 42 - running_time: 89s - train_loss: 0.032 - train_iou: 0.877 - val_loss: 0.074 - val_iou: 0.868\n",
      "epoch: 43 - running_time: 89s - train_loss: 0.028 - train_iou: 0.882 - val_loss: 0.072 - val_iou: 0.871\n",
      "epoch: 44 - running_time: 88s - train_loss: 0.027 - train_iou: 0.887 - val_loss: 0.072 - val_iou: 0.875\n",
      "epoch: 45 - running_time: 88s - train_loss: 0.031 - train_iou: 0.879 - val_loss: 0.075 - val_iou: 0.875\n",
      "epoch: 46 - running_time: 88s - train_loss: 0.027 - train_iou: 0.895 - val_loss: 0.074 - val_iou: 0.876\n",
      "epoch: 47 - running_time: 88s - train_loss: 0.076 - train_iou: 0.731 - val_loss: 0.180 - val_iou: 0.836\n",
      "epoch: 48 - running_time: 88s - train_loss: 0.032 - train_iou: 0.868 - val_loss: 0.084 - val_iou: 0.857\n",
      "epoch: 49 - running_time: 88s - train_loss: 0.027 - train_iou: 0.888 - val_loss: 0.071 - val_iou: 0.874\n",
      "epoch: 50 - running_time: 88s - train_loss: 0.025 - train_iou: 0.898 - val_loss: 0.070 - val_iou: 0.882\n",
      "epoch: 51 - running_time: 87s - train_loss: 0.028 - train_iou: 0.889 - val_loss: 0.075 - val_iou: 0.874\n",
      "epoch: 52 - running_time: 87s - train_loss: 0.024 - train_iou: 0.898 - val_loss: 0.071 - val_iou: 0.882\n",
      "epoch: 53 - running_time: 88s - train_loss: 0.027 - train_iou: 0.888 - val_loss: 0.077 - val_iou: 0.864\n",
      "epoch: 54 - running_time: 88s - train_loss: 0.025 - train_iou: 0.889 - val_loss: 0.077 - val_iou: 0.874\n",
      "epoch: 55 - running_time: 88s - train_loss: 0.025 - train_iou: 0.891 - val_loss: 0.075 - val_iou: 0.874\n",
      "epoch: 56 - running_time: 88s - train_loss: 0.025 - train_iou: 0.908 - val_loss: 0.072 - val_iou: 0.877\n",
      "epoch: 57 - running_time: 87s - train_loss: 0.023 - train_iou: 0.907 - val_loss: 0.071 - val_iou: 0.884\n",
      "epoch: 58 - running_time: 87s - train_loss: 0.027 - train_iou: 0.880 - val_loss: 0.081 - val_iou: 0.858\n",
      "epoch: 59 - running_time: 87s - train_loss: 0.022 - train_iou: 0.903 - val_loss: 0.072 - val_iou: 0.869\n",
      "epoch: 60 - running_time: 87s - train_loss: 0.020 - train_iou: 0.906 - val_loss: 0.074 - val_iou: 0.886\n",
      "epoch: 61 - running_time: 87s - train_loss: 0.023 - train_iou: 0.902 - val_loss: 0.068 - val_iou: 0.884\n",
      "epoch: 62 - running_time: 87s - train_loss: 0.020 - train_iou: 0.904 - val_loss: 0.074 - val_iou: 0.876\n",
      "epoch: 63 - running_time: 88s - train_loss: 0.021 - train_iou: 0.910 - val_loss: 0.071 - val_iou: 0.885\n",
      "epoch: 64 - running_time: 87s - train_loss: 0.026 - train_iou: 0.905 - val_loss: 0.073 - val_iou: 0.891\n",
      "epoch: 65 - running_time: 87s - train_loss: 0.020 - train_iou: 0.917 - val_loss: 0.073 - val_iou: 0.890\n",
      "epoch: 66 - running_time: 88s - train_loss: 0.020 - train_iou: 0.909 - val_loss: 0.075 - val_iou: 0.887\n",
      "epoch: 67 - running_time: 87s - train_loss: 0.019 - train_iou: 0.903 - val_loss: 0.075 - val_iou: 0.886\n",
      "epoch: 68 - running_time: 87s - train_loss: 0.030 - train_iou: 0.873 - val_loss: 0.087 - val_iou: 0.853\n",
      "epoch: 69 - running_time: 87s - train_loss: 0.029 - train_iou: 0.882 - val_loss: 0.083 - val_iou: 0.864\n",
      "epoch: 70 - running_time: 87s - train_loss: 0.025 - train_iou: 0.878 - val_loss: 0.081 - val_iou: 0.868\n",
      "epoch: 71 - running_time: 87s - train_loss: 0.020 - train_iou: 0.907 - val_loss: 0.072 - val_iou: 0.884\n",
      "epoch: 72 - running_time: 87s - train_loss: 0.025 - train_iou: 0.895 - val_loss: 0.076 - val_iou: 0.868\n",
      "epoch: 73 - running_time: 87s - train_loss: 0.049 - train_iou: 0.771 - val_loss: 0.095 - val_iou: 0.804\n",
      "epoch: 74 - running_time: 87s - train_loss: 0.021 - train_iou: 0.895 - val_loss: 0.073 - val_iou: 0.876\n",
      "epoch: 75 - running_time: 87s - train_loss: 0.020 - train_iou: 0.910 - val_loss: 0.071 - val_iou: 0.888\n",
      "epoch: 76 - running_time: 87s - train_loss: 0.020 - train_iou: 0.910 - val_loss: 0.073 - val_iou: 0.887\n",
      "epoch: 77 - running_time: 87s - train_loss: 0.020 - train_iou: 0.915 - val_loss: 0.072 - val_iou: 0.897\n",
      "epoch: 78 - running_time: 87s - train_loss: 0.020 - train_iou: 0.912 - val_loss: 0.075 - val_iou: 0.884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 79 - running_time: 87s - train_loss: 0.018 - train_iou: 0.916 - val_loss: 0.074 - val_iou: 0.893\n",
      "epoch: 80 - running_time: 87s - train_loss: 0.017 - train_iou: 0.911 - val_loss: 0.077 - val_iou: 0.888\n",
      "epoch: 81 - running_time: 87s - train_loss: 0.019 - train_iou: 0.912 - val_loss: 0.075 - val_iou: 0.889\n",
      "epoch: 82 - running_time: 87s - train_loss: 0.045 - train_iou: 0.814 - val_loss: 0.097 - val_iou: 0.810\n",
      "epoch: 83 - running_time: 88s - train_loss: 0.019 - train_iou: 0.903 - val_loss: 0.075 - val_iou: 0.878\n",
      "epoch: 84 - running_time: 87s - train_loss: 0.017 - train_iou: 0.914 - val_loss: 0.078 - val_iou: 0.886\n",
      "epoch: 85 - running_time: 87s - train_loss: 0.034 - train_iou: 0.867 - val_loss: 0.079 - val_iou: 0.854\n",
      "epoch: 86 - running_time: 87s - train_loss: 0.021 - train_iou: 0.899 - val_loss: 0.081 - val_iou: 0.875\n",
      "epoch: 87 - running_time: 87s - train_loss: 0.018 - train_iou: 0.907 - val_loss: 0.073 - val_iou: 0.879\n",
      "epoch: 88 - running_time: 87s - train_loss: 0.015 - train_iou: 0.911 - val_loss: 0.073 - val_iou: 0.894\n",
      "epoch: 89 - running_time: 87s - train_loss: 0.014 - train_iou: 0.915 - val_loss: 0.077 - val_iou: 0.874\n",
      "epoch: 90 - running_time: 87s - train_loss: 0.015 - train_iou: 0.916 - val_loss: 0.078 - val_iou: 0.890\n",
      "epoch: 91 - running_time: 87s - train_loss: 0.015 - train_iou: 0.920 - val_loss: 0.075 - val_iou: 0.900\n",
      "epoch: 92 - running_time: 87s - train_loss: 0.012 - train_iou: 0.924 - val_loss: 0.075 - val_iou: 0.896\n",
      "epoch: 93 - running_time: 87s - train_loss: 0.014 - train_iou: 0.924 - val_loss: 0.075 - val_iou: 0.897\n",
      "epoch: 94 - running_time: 87s - train_loss: 0.017 - train_iou: 0.904 - val_loss: 0.077 - val_iou: 0.895\n",
      "epoch: 95 - running_time: 87s - train_loss: 0.017 - train_iou: 0.922 - val_loss: 0.072 - val_iou: 0.899\n",
      "epoch: 96 - running_time: 87s - train_loss: 0.016 - train_iou: 0.927 - val_loss: 0.071 - val_iou: 0.901\n",
      "epoch: 97 - running_time: 87s - train_loss: 0.013 - train_iou: 0.923 - val_loss: 0.073 - val_iou: 0.895\n",
      "epoch: 98 - running_time: 87s - train_loss: 0.014 - train_iou: 0.916 - val_loss: 0.076 - val_iou: 0.895\n",
      "epoch: 99 - running_time: 87s - train_loss: 0.013 - train_iou: 0.919 - val_loss: 0.075 - val_iou: 0.896\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        for batch in range(len(X_train)//batch_size):\n",
    "            X_batch = X_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            train_total_loss, train_iou_m,_ = sess.run([loss, iou_metric, train_op], {images:X_batch, labels:y_batch, is_training:True})            \n",
    "        end_time = time.time()\n",
    "        \n",
    "        val_loss = []\n",
    "        val_iou_ms = []\n",
    "        for batch in range(len(X_test)//batch_size):\n",
    "            val_X_batch = X_test[batch*batch_size:(batch+1)*batch_size]\n",
    "            val_y_batch = y_test[batch*batch_size:(batch+1)*batch_size]\n",
    "            total_val_loss, val_iou_m, val_predict_object, val_predict_class, val_predict_normalized_box = sess.run([loss, iou_metric, predict_object, predict_class, predict_normalized_box], \n",
    "                                                 {images:val_X_batch, labels:val_y_batch, is_training:False})\n",
    "            val_loss.append(total_val_loss)\n",
    "            val_iou_ms.append(val_iou_m)\n",
    "            \n",
    "        saver.save(sess, './model/yolo', global_step=epoch)\n",
    "        print('epoch: {} - running_time: {:.0f}s - train_loss: {:.3f} - train_iou: {:.3f} - val_loss: {:.3f} - val_iou: {:.3f}'.format(epoch, end_time - start_time, train_total_loss, train_iou_m, np.mean(val_loss), np.mean(val_iou_ms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "        tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - \\\n",
    "            max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])\n",
    "        lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - \\\n",
    "            max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])\n",
    "        inter = 0 if tb < 0 or lr < 0 else tb * lr\n",
    "        return inter / (box1[2] * box1[3] + box2[2] * box2[3] - inter)\n",
    "    \n",
    "def interpret_output(predict_object, predict_class, predict_normalized_box):\n",
    "    predict_box= predict_normalized_box*img_size\n",
    "    predict_object = np.expand_dims(predict_object, axis=-1)\n",
    "    predict_class = np.expand_dims(predict_class, axis=-2)\n",
    "    class_probs = predict_object*predict_class\n",
    "    \n",
    "    filter_mat_probs = np.array(class_probs >= 0.2, dtype='bool')\n",
    "    filter_mat_boxes = np.nonzero(filter_mat_probs)\n",
    "    boxes_filtered = predict_box[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "    class_probs_filtered = class_probs[filter_mat_probs]\n",
    "       \n",
    "    classes_num_filtered = np.argmax(\n",
    "        filter_mat_probs, axis=3)[\n",
    "        filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "\n",
    "    argsort = np.array(np.argsort(class_probs_filtered))[::-1]\n",
    "    boxes_filtered = boxes_filtered[argsort]\n",
    "    class_probs_filtered = class_probs_filtered[argsort]\n",
    "    classes_num_filtered = classes_num_filtered[argsort]\n",
    "\n",
    "    for i in range(len(boxes_filtered)):\n",
    "        if class_probs_filtered[i] == 0:\n",
    "            continue\n",
    "        for j in range(i + 1, len(boxes_filtered)):\n",
    "            if iou(boxes_filtered[i], boxes_filtered[j]) > 0.5:\n",
    "                class_probs_filtered[j] = 0.0\n",
    "\n",
    "    filter_iou = np.array(class_probs_filtered > 0.0, dtype='bool')\n",
    "    boxes_filtered = boxes_filtered[filter_iou]\n",
    "    class_probs_filtered = class_probs_filtered[filter_iou]\n",
    "    classes_num_filtered = classes_num_filtered[filter_iou]\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(boxes_filtered)):\n",
    "        result.append(\n",
    "            [classes_num_filtered[i],\n",
    "             boxes_filtered[i][0],\n",
    "             boxes_filtered[i][1],\n",
    "             boxes_filtered[i][2],\n",
    "             boxes_filtered[i][3],\n",
    "             class_probs_filtered[i]])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 15\n",
    "result = interpret_output(val_predict_object[img_idx], val_predict_class[img_idx], val_predict_normalized_box[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_result(img, result):\n",
    "    plt.figure(figsize=(10,10), dpi=40)\n",
    "    img = np.pad(img, [(50,50), (50,50), (0,0)], mode='constant', constant_values=255)\n",
    "    for i in range(len(result)):\n",
    "        x = int(result[i][1])+50\n",
    "        y = int(result[i][2])+50\n",
    "        w = int(result[i][3] / 2)\n",
    "        h = int(result[i][4] / 2)\n",
    "        cv2.rectangle(img, (x - w, y - h), (x + w, y + h), (231, 76, 60), 2)\n",
    "        cv2.rectangle(img, (x - w, y - h - 20),\n",
    "                      (x -w + 50, y - h), (46, 204, 113), -1)\n",
    "        cv2.putText(\n",
    "            img, '{} : {:.2f}'.format(result[i][0] ,result[i][5]),\n",
    "            (x - w + 5, y - h - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.3,\n",
    "            (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    #\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "            \n",
    "draw_result(val_X_batch[img_idx]*255, result)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.nonzero(val_y_batch[img_idx][:,:,0])\n",
    "# x_label =  np.concatenate([val_y_batch[img_idx][x[0], x[1]][:,:5],\n",
    "#         np.argmax(val_y_batch[img_idx][x[0], x[1]][:,5:], axis=1).reshape((5,1))],\n",
    "#                   axis=-1)\n",
    "# draw_result(val_X_batch[img_idx]*255, x_label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
